{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MANJIT SINGH T117\n",
        "\n",
        "AIM - Principal Component Analysis (PCA)\n",
        "\n",
        " Perform PCA on a dataset to reduce dimensionality.\n",
        "\n",
        " Evaluate the explained variance and select the appropriate number of principal components.\n",
        "\n",
        " Visualize the data in the reduced-dimensional space."
      ],
      "metadata": {
        "id": "vCrWUBPT8tLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1\\. Reducing Features Using Principal Components"
      ],
      "metadata": {
        "id": "coVZLC655ssM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "# We use pandas here because the data is a CSV file\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "\n",
        "# Preprocessing: The mushroom dataset is text-based, we must convert to numbers first\n",
        "le = LabelEncoder()\n",
        "for col in df.columns:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Define Feature Matrix (X) and Target Vector (y)\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Standardize the feature matrix\n",
        "X_std = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Create a PCA that will retain 99% of the variance\n",
        "pca = PCA(n_components=0.99, whiten=True)\n",
        "\n",
        "# Conduct PCA\n",
        "X_pca = pca.fit_transform(X_std)\n",
        "\n",
        "# Show results\n",
        "print('Original number of features:', X.shape[1])\n",
        "print('Reduced number of features:', X_pca.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA3SgKGC8I_H",
        "outputId": "dde713ce-01c1-4bfc-969c-113acb654020"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 30\n",
            "Reduced number of features: 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2\\. Reducing Features When Data Is Linearly Inseparable"
      ],
      "metadata": {
        "id": "olOjY0OE5ssQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Load the data (Using the X_std created in the previous step)\n",
        "# We will take a subset of 1000 samples to speed up processing,\n",
        "# as Kernel PCA creates an N x N matrix which can be slow on large datasets.\n",
        "X_subset = X_std[:1000]\n",
        "\n",
        "# Apply kernel PCA with radius basis function (RBF) kernel\n",
        "kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n",
        "X_kpca = kpca.fit_transform(X_subset)\n",
        "\n",
        "# Show results\n",
        "print('Original number of features:', X_subset.shape[1])\n",
        "print('Reduced number of features:', X_kpca.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nvMhrK674cW",
        "outputId": "a920ed15-6724-4d1a-84b8-aecf551cd45d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 30\n",
            "Reduced number of features: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3\\. Reducing Features by Maximizing Class Separability"
      ],
      "metadata": {
        "id": "tzOuHpRQ5ssR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Create an LDA that will reduce the data down to 1 feature\n",
        "lda = LinearDiscriminantAnalysis(n_components=1)\n",
        "\n",
        "# run an LDA and use it to transform the features\n",
        "# LDA is supervised, so we must pass y (the class targets)\n",
        "X_lda = lda.fit(X_std, y).transform(X_std)\n",
        "\n",
        "# Print the number of features\n",
        "print('Original number of features:', X_std.shape[1])\n",
        "print('Reduced number of features:', X_lda.shape[1])\n",
        "\n",
        "# View the ratio of explained variance\n",
        "print('Explained variance ratio:', lda.explained_variance_ratio_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9ooW-8T7_lc",
        "outputId": "f0b8c1a8-6f9b-4ad8-910b-5c0c9445615b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 30\n",
            "Reduced number of features: 1\n",
            "Explained variance ratio: [0.99725161]\n"
          ]
        }
      ]
    }
  ]
}